########################################################################
# This is the configuration file of GridPilot
#
# Sections:
#
# - GridPilot : contains general parameters
# - File transfer systems : information about the file transfer systems
# - - For each system defined in File transfer systems, a section is required
# - Databases : information about the database connection
# - - For each system defined in Databases, a section is required
# - Computing Systems : contains the list of computing systems
# - - For each system defined in section Computing Systems, a section is required
#
# All tags are case insensitive, and a text after a # is a comment (but \# is allowed)
#
# Main author:
#
# - Frederik ORELLANA (frederik.orellana@cern.ch)
#
# Contributors:
#
# - Cyril TOPFEL (cyril.topfel@lhep.unibe.ch)
# - Marco NIINIMAKI (marko.niinimaki@cern.ch)
# - Luc GOOSSENS (luc.goossens@cern.ch)
# - Vandy BERTEN (vandy.berten@cern.ch)
#
########################################################################

[GridPilot]
# This name is used for field in jobDefinition tables ONLY if
# no name can be obtained from grid the certificate.
default user = firelord
# Path to images, certificates, about.
resources = /resources/
# Debug level ; 0 : all debugs are disabled, 3 : maximum.
# Notice that having a high debug level can have a big impact on performance.
debug = 2

# Inital panels (tabs) displayed.
initial panels = transformation dataset

# File to store the history of the file browser.
browser history file = ~/.gridpilot_history.txt

# Default URL when downloading/replicating, use ~ for local homedir, e.g. ~/GridPilot/
# Remote gsiftp URLs are also supported, e.g. gsiftp://my.home.server/grid/users/user_name/
grid home url = ~/GridPilot/data/

# URLs of RTE catalogs. They are used by computing system plugins
# GridFactory and VMFork to populate the runtimeEnvironment table
# of the runtime databases. They can be http URLs or local paths.
#later->runtime catalog URLs = http://www.gridfactory.org/rtes/rtes.rdf
runtime catalog URLs = http://gridfactory.nbi.dk/rtes/rtes.rdf

# Directory with runtime setup scripts.
runtime directory = ~/GridPilot/runtimeEnvironments

# Number of file records to diplay on files tabs (with previous/next buttons).
# Use -1 to show all files.
file rows = 500

# Web proxy host IP name or address.
proxy host = 
# Web proxy port number.
proxy port =

# List of preferred files servers when downloading/uploading.
preferred file servers = gsiftp://lscf.nbi.dk srm://se01-lcg.projects.cscs.ch *.dk *.de *.ch *.fr

# Number of threads checking job or transfer status.
maximum simultaneous checking = 3

# Start/default value of delay between automatic checks of jobs or transfers status
# - in milliseconds.
# This value is used when the "Refresh each" checkbox is checked.
time between checks = 500

# Confirm interruption of hanging threads.
ask before thread interrupt = yes

# Attributes always displayed when creating job definitions.
job attributes = number name

## Authentication
# Proxy type, can be one of OLD, GLOBUS, RFC.
# Notice that gLite only supports OLD, ARC both and GridFactory only RFC
# (GridFactory prefers standard SSL).
proxy type = RFC
# Number of seconds before timeout of proxy when proxy will be renewed.
proxy time left limit = 43200 # 12h
# Validity of proxy in seconds.
proxy time valid = 129600 # 36 h
# Location of X509 user key.
key file = ~/.globus/userkey.pem
# Encryption password for the X509 key.
# It is not recommended to set this, since this file is stored in plain-text.
key password = 
# Location of X509 user certificate.
certificate file = ~/.globus/usercert.pem
# Directory to store the X509 proxy.
proxy directory = ~/.globus
# A comma separated list of fully qualified file or directory paths.
# If this is left blank, the default certificates shipped with GridPilot will be used.
ca certificates = 
# The name of the virtual organization under which jobs will run.
# Used for generating VOMS proxies and by GliteComputingSystem for the VirtualOrganisation tag in JDL.
# Notice that if this is not given or VOMS proxy generation fails for some other reason, a normal
# grid proxy will be attempted generated.
virtual organization = atlas
# IP name or address of VOMS server - optional; if not given a normal proxy is generated
voms server = https://lcg-voms.cern.ch:15001/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch
# Path of VOMS directory. If this is left blank, the default VOMS definitions shipped with
# GridPilot will be used.
voms directory = # /home/fjob/eclipse/workspace/GridPilot/resources/vomsdir
# VOMS fully qualified attribute name - optional
voms fqan = 

## Timeouts
# Values in seconds - 0 means wait forever
default timeout = 60
# Downloading and booting up a VM can take some time.
submit timeout = 720
updateStatus timeout = 120
killJob timeout = 60
clearOutputMapping timeout = 60
exit timeout = 60
getFullStatus timeout = 120
getCurrentOutputs timeout =  120
db timeout = 60

#####################################################################
[File transfer systems]
#
# File transfer systems used for replicating files.
#

# These are the supported systems. The names must match the protol names.
# NOTICE: always put subsystems before high-level systems, that is, always put srm last.
Systems = https gsiftp srm srm2 sss

# These ports must be inbound open in your firewall (comma-separated list).
globus tcp port range = 9001,9010

# Number of times to retry transfers.
copy retries = 0

# Timeout for copy attempts (in seconds)
copy retry timeout = 120

# Maximum number of running transfers. When this is reached,
# queued transfers will not be started until running transfers end.
maximum simultaneous running = 10
# Delay between starting transfers in milliseconds.
time between transfers = 500
# Whether to start transfers in the order they appear in the table or in random order.
randomized transfers = no

[https]
class = gridpilot.ftplugins.https.HTTPSFileTransfer
enabled = yes
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 3

[gsiftp]
class = gridpilot.ftplugins.gsiftp.GSIFTPFileTransfer
enabled = yes
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 3

[srm]
class = gridpilot.ftplugins.srm.SRMFileTransfer
enabled = yes
# Retries to get "Ready" on a submitted transfer, before timing out
submit check retries = 3
# Time between retries in miliseconds.
submit check sleep = 10000
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 1

[srm2]
class = gridpilot.ftplugins.srm.SRM2FileTransfer
enabled = yes
# Retries to get "Ready" on a submitted transfer, before timing out
submit check retries = 3
# Time between retries in miliseconds.
submit check sleep = 10000
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 1

[sss]
#
# The Simple Storage System (s3) offered by Amazon
#
class = gridpilot.ftplugins.sss.SSSFileTransfer
enabled = yes
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 1
# The access key id found on your AWS account web page
aws access key id = 
# The secret access key found on your AWS account web page
aws secret access key = 
# Whether or not to make uploaded files world readable
world readable files = no
# Whether or not to compress uploads
compress uploads = no
# Password for encrypting transferred files. If left empty, no encryption is done.
encryption password =
# Whether or not to use the convention of S3Fox for directories:
# If set to true, _$folder$ is appended to the name (both GridPilot and
# S3Fox suppres this string in the display).
# If set to false, nothing is appended and the content-type is simply
# set to x-directory. Unfortunately S3Fox does not use content-type.
s3fox directory mode = true

#####################################################################
[Databases]
#
# Databases used for storing runtime and transformation
# informations as well as file and job information.
#

# NOTICE: use ONLY ONE local database at a time
Systems = My_DB_Local GP_DB Regional_DB My_DB_Remote ATLAS 

#--------------------------------------------------------------------
[My_DB_Local]
#
# Minimal dataset/job/file catalog for personal production/analysis.
#

class = gridpilot.dbplugins.hsqldb.HSQLDBDatabase
driver = org.hsqldb.jdbcDriver
database = hsql://localhost/~/GridPilot/My_DB
user = sa
password = ""

parameters = driver database user password

description = Local dataset/file/job database
enabled = yes

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default jobDefinition fields = name status
default file fields = dsname lfname pfname guid
file identifier = guid
file name = lfname
file dataset reference = name dsname
# The field on the displayed file table, containing the PFN(s)
pfns field = pfname
bytes field = fsize
checksum field = md5sum

dataset field names = identifier,                       name,                 transformationName, transformationVersion, metaData,         runNumber,   totalFiles,  totalEvents, created,  lastModified, inputDataset,     inputDB,          outputLocation
dataset field types = LONGVARCHAR NOT NULL PRIMARY KEY, LONGVARCHAR NOT NULL, LONGVARCHAR NULL,   LONGVARCHAR NULL,      LONGVARCHAR NULL, BIGINT NULL, BIGINT NULL, BIGINT NULL, DATETIME, DATETIME,     LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR NULL

jobDefinition field names = identifier,                                           datasetName, name,        guid,        number, eventMin, eventMax, nEvents,  outputFileBytes, outputFileChecksum, cpuSeconds, status,  userInfo,    inputFileURLs, transPars,   outFileMapping, providerInfo,      stdoutDest,  stderrDest,  created,  lastModified, jobID,        outTmp,       errTmp,       validationResult, computingSystem, csStatus,     metaData
jobDefinition field types = INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, INT,    INT,      INT,      INT,      INT NULL,        VARCHAR,            INT NULL,   VARCHAR, LONGVARCHAR, LONGVARCHAR,   LONGVARCHAR, LONGVARCHAR,    LONGVARCHAR,       LONGVARCHAR, LONGVARCHAR, DATETIME, DATETIME,     VARCHAR NULL, VARCHAR NULL, VARCHAR NULL, LONGVARCHAR NULL, VARCHAR NULL,    VARCHAR NULL, LONGVARCHAR NULL

transformation field names = identifier, name, version, runtimeEnvironmentName, arguments, outputFiles, script, comment, created, lastModified, inputFiles
transformation field types = INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, LONGVARCHAR, VARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, DATETIME, DATETIME, LONGVARCHAR

runtimeEnvironment field names = identifier, name, computingSystem, certificate, url, initLines, depends, provides, created, lastModified
runtimeEnvironment field types = INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, DATETIME, DATETIME

# This is to have an explicit file catalog

file field names = lfname, pfname, filetype, md5sum, fsize, lastmodified, archival, dsname, guid

t_lfn field names = lfname, guid
t_lfn field types = LONGVARCHAR, VARCHAR NULL

t_pfn field names = pfname, guid, filetype
t_pfn field types = LONGVARCHAR, VARCHAR NULL, LONGVARCHAR NULL

t_meta field names = guid, md5sum, fsize, lastmodified, archival, dsname
t_meta field types = VARCHAR, LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR, VARCHAR

#--------------------------------------------------------------------
[My_DB_Remote]
#
# Minimal dataset/job catalog for personal production/analysis
# The database and user name are deduced from the user certificate.
# The authentication is also done with the user certificate.
# An explicit file catalog should not be needed.
#

class = gridpilot.dbplugins.mysql.MySQLDatabase
driver = org.gjt.mm.mysql.Driver
database = jdbc:mysql://my.home.server:3306/

parameters = driver database user password

description = Remote personal dataset/job database
enabled = no

# JDBC connect timeouts in milliseconds; 0 means no timeout
connect timeout = 0
# JDBC socket timeouts in milliseconds; 0 means no timeout
socket timeout = 0

# Whether or not to cache search results locally
cache search results = yes

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default jobDefinition fields = name status
default file fields = name datasetName url

file name = fileName
pfns field = pfname
bytes field = bytes
checksum field = checksum

# NOTICE: semi-colon will be replaced with comma after parsing the fields

dataset field names = identifier,                   name,                                     transformationName, transformationVersion, metaData,  runNumber,    totalFiles,   totalEvents,  created,  lastModified, inputDataset,      inputDB,           outputLocation
dataset field types = VARCHAR(255) NOT NULL UNIQUE, VARCHAR(255) NOT NULL UNIQUE PRIMARY KEY, VARCHAR(255) NULL,  VARCHAR(255) NULL,     TEXT NULL, INT(20) NULL, INT(20) NULL, INT(20) NULL, DATETIME, DATETIME,     VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL

jobDefinition field names = identifier, datasetName, name, guid, number, eventMin, eventMax, nEvents, outputFileBytes, outputFileChecksum, cpuSeconds, status, userInfo, inputFileURLs, transPars, outFileMapping, providerInfo, stdoutDest, stderrDest, created, lastModified, jobID, outTmp, errTmp, validationResult, computingSystem, csStatus, metaData
jobDefinition field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, VARCHAR(255), INT(16) NULL, ENUM('Defined'; 'Submitted'; 'Validated'; 'Failed'; 'Undecided'; 'Aborted') NULL, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(2048), VARCHAR(255), VARCHAR(255), VARCHAR(255), DATETIME, DATETIME, VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL

transformation field names = identifier, name, version, runtimeEnvironmentName, arguments, outputFiles, script, comment, created, lastModified, inputFiles
transformation field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), TEXT, DATETIME, DATETIME, VARCHAR(255)

runtimeEnvironment field names = identifier, name, computingSystem, certificate, url, initLines, depends, provides, created, lastModified
runtimeEnvironment field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), TEXT, VARCHAR(255), TEXT, TEXT, TEXT, DATETIME, DATETIME

#--------------------------------------------------------------------
[Regional_DB]
#
# Regional dataset and file catalog.
# Authentication is done with the grid certificate (user/password empty).
# Read access to non-authorized users is typically granted with user
# 'dq2user' and password 'dqpwd'.
#

class = gridpilot.dbplugins.mysql.MySQLDatabase
driver = org.gjt.mm.mysql.Driver
database = jdbc:mysql://www.gridpilot.dk:3306/file_catalog

parameters = driver database user password

description = Regional dataset/file catalog
enabled = no

# JDBC connect timeouts in milliseconds; 0 means no timeout
connect timeout = 0
# JDBC socket timeouts in milliseconds; 0 means no timeout
socket timeout = 0

# Whether or not to cache search results locally
cache search results = yes

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default file fields = dsname lfname pfname guid
hidden file fields = sync
file identifier = guid
file name = lfname
file dataset reference = name dsname

# NOTICE: semi-colon will be replaced with comma after parsing the fields

dataset field names = identifier,                   name,                                     transformationName, transformationVersion, metaData,      runNumber,    totalFiles,   totalEvents,  created,  lastModified, inputDataset,      inputDB,           outputLocation
dataset field types = VARCHAR(255) NOT NULL UNIQUE, VARCHAR(255) NOT NULL UNIQUE PRIMARY KEY, VARCHAR(255) NULL,  VARCHAR(255) NULL,     TEXT NULL, INT(20) NULL, INT(20) NULL, INT(20) NULL, DATETIME, DATETIME,     VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL

# This is to have an explicit file catalog

file field names = lfname, pfname, filetype, md5sum, fsize, lastmodified, archival, dsname, guid
pfns field = pfname
bytes field = fsize
checksum field = md5sum

t_lfn field names = lfname, guid
t_lfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL

t_pfn field names = pfname, guid, filetype
t_pfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL, VARCHAR(250) DEFAULT NULL

t_meta field names = guid, md5sum, fsize, lastmodified, archival, dsname, sync
t_meta field types = VARCHAR(40) PRIMARY KEY, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, VARCHAR(250), VARCHAR(40)

#--------------------------------------------------------------------
[GP_DB]
#
# Minimal shared dataset/job/file catalog for public testing.
# All certificates from known CAs are granted r/w access.
#

class = gridpilot.dbplugins.mysql.MySQLDatabase
driver = org.gjt.mm.mysql.Driver
database = jdbc:mysql://www.gridpilot.dk:3306/gridpilot

parameters = driver database user password

description = Public dataset/job/file database
enabled = no

# JDBC connect timeouts in milliseconds; 0 means no timeout
connect timeout = 0
# JDBC socket timeouts in milliseconds; 0 means no timeout
socket timeout = 0

# Whether or not to cache search results locally
cache search results = no

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default jobDefinition fields = name status
default file fields = dsname lfname pfname guid
file identifier = guid
file name = lfname
file dataset reference = name dsname
pfns field = pfname
bytes field = fsize
checksum field = md5sum

# NOTICE: semi-colon will be replaced with comma after parsing the fields

dataset field names = identifier,                   name,                                     transformationName, transformationVersion, metaData,  runNumber,    totalFiles,   totalEvents,  created,  lastModified, inputDataset,      inputDB,           outputLocation
dataset field types = VARCHAR(255) NOT NULL UNIQUE, VARCHAR(255) NOT NULL UNIQUE PRIMARY KEY, VARCHAR(255) NULL,  VARCHAR(255) NULL,     TEXT NULL, INT(20) NULL, INT(20) NULL, INT(20) NULL, DATETIME, DATETIME,     VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL

jobDefinition field names = identifier, datasetName, name, guid, number, eventMin, eventMax, nEvents, outputFileBytes, outputFileChecksum, cpuSeconds, status, userInfo, inputFileURLs, transPars, outFileMapping, providerInfo, stdoutDest, stderrDest, created, lastModified, jobID, outTmp, errTmp, validationResult, computingSystem, csStatus, metaData
jobDefinition field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, VARCHAR(255), INT(16) NULL, ENUM('Defined'; 'Submitted'; 'Validated'; 'Failed'; 'Undecided'; 'Aborted') NULL, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(2048), VARCHAR(255), VARCHAR(255), VARCHAR(255), DATETIME, DATETIME, VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL

transformation field names = identifier, name, version, runtimeEnvironmentName, arguments, outputFiles, script, comment, created, lastModified, inputFiles
transformation field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), TEXT, DATETIME, DATETIME, VARCHAR(255)

runtimeEnvironment field names = identifier, name, computingSystem, certificate, url, initLines, depends, provides, created, lastModified
runtimeEnvironment field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), TEXT, VARCHAR(255), TEXT, TEXT, TEXT, DATETIME, DATETIME

# This is to have an explicit file catalog

file field names = lfname, pfname, filetype, md5sum, fsize, lastmodified, archival, dsname, guid
pfns field = pfname

t_lfn field names = lfname, guid
t_lfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL

t_pfn field names = pfname, guid, filetype
t_pfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL, VARCHAR(250) DEFAULT NULL

t_meta field names = guid, md5sum, fsize, lastmodified, archival, dsname, sync
t_meta field types = VARCHAR(40) PRIMARY KEY, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, VARCHAR(250), VARCHAR(40)

#--------------------------------------------------------------------
[ATLAS]
#
# Central ATLAS dataset and file catalog.
# The dataset catalog is the DQ2 server at CERN.
# The file catalog is a combination of the DQ2 server
# at CERN and the LFC and MySQL catalogs registered as
# sites in the TiersOfAtlas file at CERN. Writing file
# information is done by writing to the MySQL database
# registered as alias for the home LFC server.
# Authentication and authorization on this MySQL database
# is done via the grid certificate. The MySQL database is
# synchronized with the LFC database by a cron job running
# on the MySQL server.
#
class = gridpilot.dbplugins.atlas.ATLASDatabase

parameters = 

description = Central ATLAS dataset/file catalog
enabled = no

# Whether or not to cache search results locally
cache search results = yes

# The "home" LFC or MySQL server (its alias from TOA) and, for an LFC server,
# its optional MySQL URL alias; if the "home" server is an LFC server and
# no MySQL alias is given, there is only read access.
# If a MySQL alias is given, write access is obtained by not specifying any
# user:password in the URL. In this case the grid certificate is used for authentication.
#home site = FZKDISK mysql://dq2user:dqpwd@grid00.unige.ch:3306/localreplicas
home site = # FZKDISK mysql://my.home.server:3306/localreplicas
# If the home site is an LFC site, the path under which new datasets should be
# registered should be specified. Notice that it will be prepended with /grid/atlas.
user path = /users/MyName
# When a file has several physical replicas, try these locations first.
preferred sites = NDGFT1DISK CSCS FZKDISK LYONDISK CERNCAF CERNPROD
# Sites we know are not responding.
ignored sites = NIKHEF PICDISK RALDISK TRIUMFDISK
# File catalog timeout in miliseconds
file catalog timeout = 10000

default dataset fields = dsn incomplete complete vuid
dataset identifier = vuid
dataset name = dsn
# Notice: because of DQ2 limitations: if you want to be able to download/replicate files
#         this list MUST be the full list: dsn lfn pfns guid
default file fields = dsn lfn catalogs pfns guid bytes checksum
file identifier = guid
file name = lfn
file dataset reference = dsn dsn
pfns field = pfns
bytes field = bytes
checksum field = checksum
# The file used to resolve site names to file catalog servers.
tiers of atlas = http://atlas.web.cern.ch/Atlas/GROUPS/DATABASE/project/ddm/releases/TiersOfATLASCache.py
#tiers of atlas = file:~/GridPilot/TiersOfATLASCache.txt
# If forceDelete is set to true, files will be attempted deleted on
# all physical locations and on the home catalog server MySQL alias
# and the home server will be de-registered in DQ, even if other
# catalog sites are registered in DQ than the home catalog or if there
# is no home catalog set.
force file deletion = no

## DQ2 parameters
DQ2 server = atlddmcat.cern.ch
DQ2 port = 80
DQ2 secure port = 443
DQ2 path = /dq2

#####################################################################
[Computing systems]
#
# Backends for executing computing jobs.
#

Systems = Fork VMFork EC2 GridFactory NG GLite # SSH SSH_Pool EC2Soap EC2Alt

# Maximum total number of simultaneous submission threads.
maximum simultaneous submissions = 3
# Maximum total number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 10
# Delay between starting submissions (in milliseconds).
time between submissions = 1000 # in ms
# Whether to start submissions in the order they appear in the table
# or in random order.
randomized submission = no
# Delay between starting validation (in milliseconds)
delay before validation = 3000
# Maximum number of simultaneous validation threads.
maximum simultaneous validating = 3

#--------------------------------------------------------------------
[Fork]
#
# Run jobs on the local computer.
#

## Sub-section for GridPilot
enabled = yes
host = localhost
user =
password =
class = gridpilot.csplugins.fork.ForkComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking)
max jobs by update = 10
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 1
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 1

## Sub-section for this plug-in
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fall-back shell command for getting remote input files.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DBs to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local

# Certificate used by submitters to encrypt their
# proxy certificate+key to the GridPilot picking up the job.
# Only needed to pick up jobs.
# NOTICE: the certificate should NOT include a key!
public certificate = ~/.globus/usercert.pem

#--------------------------------------------------------------------
[VMFork]
#
# Run jobs inside virtual machine on the local computer.
#

## Sub-section for GridPilot
enabled = no
host = localhost
user =
password =
class = gridpilot.csplugins.vmfork.VMForkComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking)
max jobs by update = 10
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 1
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 1

## Sub-section for this plug-in
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# In which DBs to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local
# Whether or not to force running jobs inside a virtual machine. If set
# to false, jobs will run directly on the local machine if possible
# (if all required RTEs can be installed).
enforce virtualization = yes
# The amount of RAM (in megabytes) to assign to the virtual machine.
default ram per vm = 512
# Command to start a terminal running a secure shell. This command will be
# given the port number as first argument and user@host as second argument.
ssh command = /usr/bin/konsole -e /usr/bin/ssh -X -p
# Time to wait (in seconds) for a VM to boot. Notice, that for a first boot
# this includes download time of all required RTEs and typically also 
# duplication of the disk image.
boot timeout = 700

#--------------------------------------------------------------------
[GridFactory]
#
# Submit jobs to a GridPilot database somewhere,
# where they will be picked up by other GridPilots.
#

## Sub-Section for GridPilot
enabled = no
host = localhost
user = 
password =
class = gridpilot.csplugins.gridfactory.GridFactoryComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 40
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 10

## Sub-Section for this plug-in
# Where to submit the jobs.
submission url = https://gridfactory.nbi.dk/gridfactory/jobs/
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local
# Local directory to keep script and stdout/stderr
working directory = ~/GridPilot/jobs

# Space separated list of certificate subjects or URLs (http, ftp, gsiftp, srm, file)
# of files containing certificate subjects, e.g. 'http://www.chipp.ch/vo/chipp.txt'.
# Each list entry must be enclosed in single-quotes.
# The owner of a certificates with one these subjects will be allowed
# to run jobs, if the CA that issued the certificate is on the list of ca certificates.
# If empty, any provider will be allowed to run my jobs.
allowed subjects =
# Whether jobs should run on Windows (or Linux/UNIX).
on windows = false
# Whether or not jobs should be forced to run inside virtual machines.
virtualize = false

#--------------------------------------------------------------------
[SSH]
#
# Run jobs on remote computer.
#

## Sub-section for GridPilot
enabled = no
# Notice: if you use lxplus.cern.ch you disconnect, reconnect and find you jobs again,
# since you will most likely be on a different machine.
host = lxplus203.cern.ch
user = 
password =
class = gridpilot.csplugins.fork.ForkComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 1
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 1

## Sub-Section for this plug-in
# Optionally login with a private key. This will be tried only once, then
# normal login with password will be tried
ssh key file = 
ssh key passphrase = 
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local

#--------------------------------------------------------------------
[SSH_Pool]
#
# Run jobs on a pool of remote computers.
#

## Sub-section for GridPilot
enabled = no
# The main host, where runtime environments, etc. are present.
host = localhost
user = 
password =
class = gridpilot.csplugins.forkpool.ForkPoolComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 10

## Sub-Section for this plug-in
# Optionally login with a private key. This will be tried only once, then
# normal login with password will be tried
ssh key file = 
ssh key passphrase = 
# Additional hosts for running jobs
hosts = lxplus218.cern.ch lxplus219.cern.ch
# List of user names (one per host). If empty prompting will be done.
users = 
# List of passwords (one per host). If empty prompting will be done.
passwords =
# Maximum number of running jobs (one number for each host). If empty, 1's will be assumed.
max running jobs =
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local

#--------------------------------------------------------------------
[EC2]
#
# Run jobs on a pool of virtual machines in the Amazon Elastic
# Compute Cloud.
#

## Sub-section for GridPilot
enabled = no
# host should just be localhost. It is used for manipulating local job
# information, etc.
host = localhost
# user should be left blank - we're on localhost
user = 
# password should be left blank - we're on localhost
password =
class = gridpilot.csplugins.ec2.EC2ComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 10

## Sub-Section for this plug-in
# IP address of the server
server address = 
# Port the web service is running on
port number = 
# Whether to use HTTPS or HTTP
secure = yes
# Path to the web service
service path = 
# The access key id found on your AWS account web page
aws access key id =
# The secret access key found on your AWS account web page
aws secret access key =
# Fall-back AWS Machine Image (AMI) to use. If this is not set, jobs with
# OS requirements that cannot be met are not run (no machine is booted).
fallback ami id =
# The subnet from which ssh access to the AWS virtual machines will
# be granted. If left empty, 0.0.0.0/0 is used, i.e. access is granted from anywhere
ssh access subnet = 0.0.0.0/0
# Maximum number of virtual machines to be run
maximum machines = 2
# Number of jobs per machine.
jobs per machine = 1
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# RTE catalog with AMIs providing software installations
runtime catalog URL = sss://gridpilot/ec2_rtes.rdf
# Prefix of AMI manifests. If specified, only manifests starting with this
# will be considered.
ami prefix = gridpilot/
runtime catalog URL = sss://gridpilot/ec2_rtes.rdf
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local
# Command to start a terminal running a secure shell. This command will
# be given the full path of the SSH key as first argument and the host
# name as second argument.
ssh command = /usr/bin/konsole -e /usr/bin/ssh -X -l root -i

#--------------------------------------------------------------------
[EC2Soap]
#
# Run jobs on a pool of virtual machines in the Amazon Elastic
# Compute Cloud.
#

## Sub-section for GridPilot
enabled = no
# host should just be localhost. It is used for manipulating local job
# information, etc.
host = localhost
# user should be left blank - we're on localhost
user =
# password should be left blank - we're on localhost
password =
class = gridpilot.csplugins.ec2soap.EC2SoapComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 10

## Sub-Section for this plug-in
# URL the EC2 service
service url = http://mayhem9.cs.ucsb.edu:8773/services/Eucalyptus # https://ec2.amazonaws.com # 
# The AWS Machine Image (AMI) to use
fallback ami id = emi-0D52023B # ami-2b5fba42 # 
# The subnet from which ssh access to the AWS virtual machines will
# be granted. If left empty, 0.0.0.0/0 is used, i.e. access is granted from anywhere
ssh access subnet = 0.0.0.0/0
# Maximum number of virtual machines to be run
maximum machines = 2
# Number of jobs per machine.
jobs per machine = 1
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local
# Command to start a terminal running a secure shell. This command will
# be given the full path of the SSH key as first argument and the host
# name as second argument.
ssh command = /usr/bin/konsole -e /usr/bin/ssh -X -l root -i

#--------------------------------------------------------------------
[EC2Alt]
#
# Run jobs on a pool of virtual machines in the Amazon Elastic
# Compute Cloud.
#

## Sub-section for GridPilot
enabled = no
# host should just be localhost. It is used for manipulating local job
# information, etc.
host = localhost
# user should be left blank - we're on localhost
user =
# password should be left blank - we're on localhost
password =
class = gridpilot.csplugins.eucalyptus.EC2AltComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 10

## Sub-Section for this plug-in
# URL the EC2 service
service url = https://ec2.amazonaws.com
# The access key id found on your AWS account web page
aws access key id = 
# The secret access key found on your AWS account web page
aws secret access key = 
# The AWS Machine Image (AMI) to use
fallback ami id = ami-2b5fba42
# The subnet from which ssh access to the AWS virtual machines will
# be granted. If left empty, 0.0.0.0/0 is used, i.e. access is granted from anywhere
ssh access subnet = 0.0.0.0/0
# Maximum number of virtual machines to be run
maximum machines = 2
# Number of jobs per machine.
jobs per machine = 1
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local
# Command to start a terminal running a secure shell. This command will
# be given the full path of the SSH key as first argument and the host
# name as second argument.
ssh command = /usr/bin/konsole -e /usr/bin/ssh -X -l root -i

#--------------------------------------------------------------------
[NG]
#
# Submit jobs to NorduGrid.
#

## Sub-section for GridPilot
enabled = no
class = gridpilot.csplugins.ng.NGComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 100

## Sub-section for this plug-in
# This is where scripts and stdout/stderr is kept on the *local* disk.
# Use / instead of \.
working directory = ~/GridPilot/jobs
# Where to write the runtime records.
runtime databases = My_DB_Local
# Whether or not to use the information system.
# If set to no, functionality is somewhat limited, but
# it should be possible to submit jobs to a specific cluster.
use information system = yes
# The GIIS information servers to query for clusters.
GIISes = ldap://index1.nordugrid.org:2135/Mds-Vo-name=nordugrid,o=grid # ldap://odin.switch.ch:2135/Mds-Vo-name=Switzerland,o=grid
# Explicitly chosen clusters. If this is set, GIIS servers will not be queried.
clusters =  lscf.nbi.dk # grid03.unige.ch lheppc50.unibe.ch lheppc10.unibe.ch nordugrid-lcg.projects.cscs.ch hypatia.uio.no hagrid.it.uu.se benedict.grid.aau.dk gateway01.dcsc.ku.dk interop.dcgc.dk morpheus.dcgc.dk
# Required CPU time in minutes.
cpu time = 10
# Required memory in megabytes
memory = 512
# Maximum number of retrying failed jobs.
max rerun = 1

#--------------------------------------------------------------------
[GLite]
#
# Submit jobs to the EGEE grid.
#

## Sub-section for GridPilot
enabled = no
class = gridpilot.csplugins.glite.GLiteComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1
# Maximum number of simultaneous submission threads submitting to this CS.
maximum simultaneous submissions = 3
# Maximum number of running jobs. When this is reached,
# queued jobs will not be started until running jobs end.
maximum simultaneous running = 100

## Sub-section for this plug-in
# This is where scripts and stdout/stderr is kept on the *local* disk.
# Use / instead of \.
working directory = ~/GridPilot/jobs
# Required CPU time in minutes.
cpu time = 180
# Required memory in megabytes
memory = 512
# Maximum number of retrying failed jobs.
max rerun = 10
# The service endpoint.
wmproxy url = https://wms111.cern.ch:7443/glite_wms_wmproxy_server # https://wms105.cern.ch:7443/glite_wms_wmproxy_server
# The BDII host.
bdii host = lcg-bdii.cern.ch
# Clusters to scan for RTEs. Leaving this empty will cause all clusters
# to be scanned - this may take a long time.
runtime clusters = ce.cyf-kr.edu.pl # ce103.cern.ch # ce01-lcg.projects.cscs.ch ce01-lcg.projects.cscs.ch g03n02.pdc.kth.se
# Space separated list of virtual organizations for which to setup RTEs.
runtime vos = ATLAS CMS
# RTE tags to be used to label replacement rules. Can be any strings.
runtime tags = atlas_production
# For each of above tags. Mapping of tag to actual setup scripts to be sourced.
# The first string must be a regular expression, the following strings must resolve to
# the script paths.
atlas_production = VO-atlas-production-(.*) \$VO_ATLAS_SW_DIR/software/$1/setup.sh \$SITEROOT/AtlasOffline/$1/AtlasOfflineRunTime/cmt/setup.sh
# Where to write the runtime records.
runtime databases = My_DB_Local

# This entry must be here for PreferencesPanel to work.
[end]
