########################################################################
# This is the configuration file of GridPilot
#
# Sections:
#
# - GridPilot : contains general parameters
# - File transfer systems : information about the file transfer systems
# - - For each system defined in File transfer systems, a section is required
# - Databases : information about the database connection
# - - For each system defined in Databases, a section is required
# - Computing Systems : contains the list of computing systems
# - - For each system defined in section Computing Systems, a section is required
#
# All tags are case insensitive, and a text after a # is a comment (but \# is allowed)
#
# Main author:
#
# - Frederik ORELLANA (frederik.orellana@cern.ch)
#
# Contributors:
#
# - Cyril TOPFEL (cyril.topfel@lhep.unibe.ch)
# - Marco NIINIMAKI (marko.niinimaki@cern.ch)
# - Luc GOOSSENS (luc.goossens@cern.ch)
# - Vandy BERTEN (vandy.berten@cern.ch)
#
########################################################################

[GridPilot]
# This name is used for field in jobDefinition tables ONLY if
# no name can be obtained from grid the certificate.
default user = firelord
# Path to images, certificates, about.
resources = /resources/
# Debug level ; 0 : all debugs are disabled, 3 : maximum.
# Notice that having a high debug level can have a big impact on performance.
debug = 2

# Inital panels (tabs) displayed.
initial panels = transformation dataset

# File to store the history of the file browser.
browser history file = ~/gridpilot_history.txt

# Default URL when downloading/replicating, use ~ for local homedir, e.g. ~/GridPilot/
# Remote gsiftp URLs are also supported, e.g. gsiftp://grid01.unige.ch/grid/users/user_name/
grid home url = ~/GridPilot/

# URLs of an RTE catalogs. They are used by the computing system plugins
# (except for GLite) to populate the runtimeEnvironment table
# of the runtime databases. They can be http URLs or local paths.
runtime catalog URLs = ~/GridPilot/rtes.rdf http://www.gridpilot.dk/rtes.rdf

# Number of file records to diplay on files tabs (with previous/next buttons).
# Use -1 to show all files.
file rows = 500

# Web proxy host IP name or address.
proxy host = 
# Web proxy port number.
proxy port =

# List of preferred files servers when downloading/uploading.
preferred file servers = gsiftp://grid00.unige.ch gsiftp://grid02.unige.ch srm://se01-lcg.projects.cscs.ch *.dk *.de *.ch *.fr

# Number of threads checking job or transfer status.
maximum simultaneous checking = 3

# Start/default value of delay between automatic checks of jobs or transfers status
# - in milliseconds.
# This value is used when the "Refresh each" checkbox is checked.
time between checks = 500

# Confirm interruption of hanging threads.
ask before thread interrupt = yes

# Attributes always displayed when creating job definitions.
job attributes = number name

## Authentication
# Number of seconds before timeout of proxy when proxy will be renewed.
proxy time left limit = 43200 # 12h
# Validity of proxy in seconds.
proxy time valid = 129600 # 36 h
# Location of X509 user key.
key file = ~/.globus/userkey.pem
# Encryption password for the X509 key.
# It is not recommended to set this, since it will be stored in plain-text.
key password = 
# Location of X509 user certificate.
certificate file = ~/.globus/usercert.pem
# Directory to store the X509 proxy.
grid proxy directory = ~/.globus
# A comma separated list of fully qualified file or directory paths.
# If this is left blank, the default certificates shipped with GridPilot will be used.
ca certificates = # ~/.globus/certificates, /etc/grid-security/certificates

## Timeouts
# Values in seconds - 0 means wait forever
default timeout = 60
submit timeout = 60
updateStatus timeout = 60
killJob timeout = 60
clearOutputMapping timeout = 60
exit timeout = 60
getFullStatus timeout = 60
getCurrentOutputs timeout =  60
db timeout = 60

## Job pulling
# Maximum number of times failed jobs are rerun
max pull rerun = 1
# Directory to cache input files for pulled jobs
pull cache directory = ~/GridPilot/cache

#####################################################################
[File transfer systems]
#
# File transfer systems used for replicating files.
#

# These are the supported systems. The names must match the protol names.
# NOTICE: always put subsystems before high-level systems, that is, always put srm last.
Systems = https gsiftp srm sss

# These ports must be inbound open in your firewall (comma-separated list).
globus tcp port range = 9001,9010

# Number of times to retry transfers.
copy retries = 0

# Timeout for copy attempts (in seconds)
copy retry timeout = 120

# Maximum number of running transfers. When this is reached,
# queued transfers will not be started until running transfers end.
maximum simultaneous transfers = 10
# Delay between starting transfers in milliseconds.
time between transfers = 500
# Whether to start transfers in the order they appear in the table or in random order.
randomized transfers = no

[https]
class = gridpilot.ftplugins.https.HTTPSFileTransfer
enabled = yes
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 3

[gsiftp]
class = gridpilot.ftplugins.gsiftp.GSIFTPFileTransfer
enabled = yes
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 3

[srm]
class = gridpilot.ftplugins.srm.SRMFileTransfer
enabled = yes
# Retries to get "Ready" on a submitted transfer, before timing out
submit check retries = 3
# Time between retries in miliseconds.
submit check sleep = 10000
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 1

[sss]
#
# The Simple Storage System (s3) offered by Amazon
#
class = gridpilot.ftplugins.sss.SSSFileTransfer
enabled = yes
# Number of transfers whose status is checked by each thread
# (see maximum simultaneous transfers).
max transfers by update = 1
# The access key id found on your AWS account web page
aws access key id = 
# The secret access key found on your AWS account web page
aws secret access key = 
# Whether or not to make uploaded files world readable
world readable files = no
# Whether or not to compress uploads
compress uploads = no
# Password for encrypting transferred files. If left empty, no encryption is done.
encryption password =
# Whether or not to use the convention of S3Fox for directories:
# If set to true, _$folder$ is appended to the name (both GridPilot and
# S3Fox suppres this string in the display).
# If set to false, nothing is appended and the content-type is simply
# set to x-directory. Unfortunately S3Fox does not use content-type.
s3fox directory mode = true

#####################################################################
[Databases]
#
# Databases used for storing runtime and transformation
# informations as well as file and job information.
#

# NOTICE: use ONLY ONE local database at a time
Systems = My_DB_Local My_DB_Remote Regional_DB ATLAS

#--------------------------------------------------------------------
[My_DB_Local]
#
# Minimal dataset/job/file catalog for personal production/analysis.
#

class = gridpilot.dbplugins.hsqldb.HSQLDBDatabase
driver = org.hsqldb.jdbcDriver
database = hsql://localhost/~/GridPilot/My_DB
user = sa
password = ""

parameters = driver database user password

description = Local dataset/file/job database
enabled = yes

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default jobDefinition fields = name status
default file fields = dsname lfname pfname guid
file identifier = guid
file name = lfname
file dataset reference = name dsname
# The field on the displayed file table, containing the PFN(s)
pfns field = pfname
bytes field = fsize
checksum field = md5sum

dataset field names = identifier,                       name,                 transformationName, transformationVersion, metaData,         runNumber,   totalFiles,  totalEvents, created,  lastModified, inputDataset,     inputDB,          outputLocation
dataset field types = LONGVARCHAR NOT NULL PRIMARY KEY, LONGVARCHAR NOT NULL, LONGVARCHAR NULL,   LONGVARCHAR NULL,      LONGVARCHAR NULL, BIGINT NULL, BIGINT NULL, BIGINT NULL, DATETIME, DATETIME,     LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR NULL

jobDefinition field names = identifier,                                           datasetName, name,        guid,        number, eventMin, eventMax, nEvents,  outputFileBytes, outputFileChecksum, cpuSeconds, status,  userInfo,    inputFileURLs, transPars,   outFileMapping, providerInfo,      stdoutDest,  stderrDest,  created,  lastModified, jobID,        outTmp,       errTmp,       validationResult, computingSystem, csStatus,     metaData
jobDefinition field types = INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, INT,    INT,      INT,      INT,      INT NULL,        VARCHAR,            INT NULL,   VARCHAR, LONGVARCHAR, LONGVARCHAR,   LONGVARCHAR, LONGVARCHAR,    LONGVARCHAR,       LONGVARCHAR, LONGVARCHAR, DATETIME, DATETIME,     VARCHAR NULL, VARCHAR NULL, VARCHAR NULL, LONGVARCHAR NULL, VARCHAR NULL,    VARCHAR NULL, LONGVARCHAR NULL

transformation field names = identifier, name, version, runtimeEnvironmentName, arguments, outputFiles, script, comment, created, lastModified, inputFiles
transformation field types = INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, LONGVARCHAR, VARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, DATETIME, DATETIME, LONGVARCHAR

runtimeEnvironment field names = identifier, name, computingSystem, certificate, url, initLines, depends, created, lastModified
runtimeEnvironment field types = INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, LONGVARCHAR, DATETIME, DATETIME

# This is to have an explicit file catalog

file field names = lfname, pfname, filetype, md5sum, fsize, lastmodified, archival, dsname, guid

t_lfn field names = lfname, guid
t_lfn field types = LONGVARCHAR, VARCHAR NULL

t_pfn field names = pfname, guid, filetype
t_pfn field types = LONGVARCHAR, VARCHAR NULL, LONGVARCHAR NULL

t_meta field names = guid, md5sum, fsize, lastmodified, archival, dsname
t_meta field types = VARCHAR, LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR NULL, LONGVARCHAR, VARCHAR

#--------------------------------------------------------------------
[My_DB_Remote]
#
# Minimal dataset/job catalog for personal production/analysis
# The database and user name are deduced from the user certificate.
# The authentication is also done with the user certificate.
# An explicit file catalog should not be needed.
#

class = gridpilot.dbplugins.mysql.MySQLDatabase
driver = org.gjt.mm.mysql.Driver
database = jdbc:mysql://grid00.unige.ch:3306/

parameters = driver database user password

description = Remote personal dataset/job database
enabled = no

# JDBC connect timeouts in milliseconds; 0 means no timeout
connect timeout = 0
# JDBC socket timeouts in milliseconds; 0 means no timeout
socket timeout = 0

# Whether or not to cache search results locally
cache search results = yes

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default jobDefinition fields = name status
default file fields = name datasetName url

file name = fileName
pfns field = pfname
bytes field = bytes
checksum field = checksum

# NOTICE: semi-colon will be replaced with comma after parsing the fields

dataset field names = identifier,                                      name,                         transformationName, transformationVersion, metaData,  runNumber,    totalFiles,   totalEvents,  created,  lastModified, inputDataset,      inputDB,           outputLocation
dataset field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255) NOT NULL UNIQUE, VARCHAR(255) NULL,  VARCHAR(255) NULL,     TEXT NULL, INT(20) NULL, INT(20) NULL, INT(20) NULL, DATETIME, DATETIME,     VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL

jobDefinition field names = identifier, datasetName, name, guid, number, eventMin, eventMax, nEvents, outputFileBytes, outputFileChecksum, cpuSeconds, status, userInfo, inputFileURLs, transPars, outFileMapping, providerInfo, stdoutDest, stderrDest, created, lastModified, jobID, outTmp, errTmp, validationResult, computingSystem, csStatus, metaData
jobDefinition field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, VARCHAR(255), INT(16) NULL, ENUM('Defined'; 'Submitted'; 'Validated'; 'Failed'; 'Undecided'; 'Aborted') NULL, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(2048), VARCHAR(255), VARCHAR(255), VARCHAR(255), DATETIME, DATETIME, VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL

transformation field names = identifier, name, version, runtimeEnvironmentName, arguments, outputFiles, script, comment, created, lastModified, inputFiles
transformation field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), TEXT, DATETIME, DATETIME, VARCHAR(255)

runtimeEnvironment field names = identifier, name, computingSystem, certificate, url, initLines, depends, created, lastModified
runtimeEnvironment field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), TEXT, VARCHAR(255), TEXT, TEXT, DATETIME, DATETIME

#--------------------------------------------------------------------
[Regional_DB]
#
# Regional dataset and file catalog.
# Authentication is done with the grid certificate (user/password empty).
# Read access to non-authorized users can be granted with user
# 'dq2user' and password 'dqpwd'.
#

class = gridpilot.dbplugins.mysql.MySQLDatabase
driver = org.gjt.mm.mysql.Driver
database = jdbc:mysql://grid00.unige.ch:3306/local_production

parameters = driver database user password

description = Regional dataset/file catalog
enabled = no

# JDBC connect timeouts in milliseconds; 0 means no timeout
connect timeout = 0
# JDBC socket timeouts in milliseconds; 0 means no timeout
socket timeout = 0

# Whether or not to cache search results locally
cache search results = yes

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default file fields = dsname lfname pfname guid
hidden file fields = sync
file identifier = guid
file name = lfname
file dataset reference = name dsname

# NOTICE: semi-colon will be replaced with comma after parsing the fields

dataset field names = identifier,                        name,                         transformationName, transformationVersion, metaData,      runNumber,    totalFiles,   totalEvents,  created,  lastModified, inputDataset,      inputDB,           outputLocation
dataset field types = VARCHAR(255) NOT NULL PRIMARY KEY, VARCHAR(255) NOT NULL UNIQUE, VARCHAR(255) NULL,  VARCHAR(255) NULL,     TEXT NULL, INT(20) NULL, INT(20) NULL, INT(20) NULL, DATETIME, DATETIME,     VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL

# This is to have an explicit file catalog

file field names = lfname, pfname, filetype, md5sum, fsize, lastmodified, archival, dsname, guid
pfns field = pfname
bytes field = fsize
checksum field = md5sum

t_lfn field names = lfname, guid
t_lfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL

t_pfn field names = pfname, guid, filetype
t_pfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL, VARCHAR(250) DEFAULT NULL

t_meta field names = guid, md5sum, fsize, lastmodified, archival, dsname, sync
t_meta field types = VARCHAR(40) PRIMARY KEY, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, VARCHAR(250), VARCHAR(40)

#--------------------------------------------------------------------
[GP_DB]
#
# Minimal dataset/job/file catalog for public production/analysis
# The database and user name are hard coded and shared.
# All certificates from known CAs are granted r/w access.
#

class = gridpilot.dbplugins.mysql.MySQLDatabase
driver = org.gjt.mm.mysql.Driver
database = jdbc:mysql://db.gridpilot.dk:3306/production

parameters = driver database user password

description = Public dataset/job/file database at gridpilot.dk
enabled = no

# JDBC connect timeouts in milliseconds; 0 means no timeout
connect timeout = 0
# JDBC socket timeouts in milliseconds; 0 means no timeout
socket timeout = 0

# Whether or not to cache search results locally
cache search results = no

default dataset fields = name outputLocation identifier
hidden dataset fields = metaData
default jobDefinition fields = name status
default file fields = dsname lfname pfname guid
file identifier = guid
file name = lfname
file dataset reference = name dsname
pfns field = pfname
bytes field = fsize
checksum field = md5sum

# NOTICE: semi-colon will be replaced with comma after parsing the fields

dataset field names = identifier,                                      name,                         transformationName, transformationVersion, metaData,  runNumber,    totalFiles,   totalEvents,  created,  lastModified, inputDataset,      inputDB,           outputLocation
dataset field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255) NOT NULL UNIQUE, VARCHAR(255) NULL,  VARCHAR(255) NULL,     TEXT NULL, INT(20) NULL, INT(20) NULL, INT(20) NULL, DATETIME, DATETIME,     VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL

jobDefinition field names = identifier, datasetName, name, guid, number, eventMin, eventMax, nEvents, outputFileBytes, outputFileChecksum, cpuSeconds, status, userInfo, inputFileURLs, transPars, outFileMapping, providerInfo, stdoutDest, stderrDest, created, lastModified, jobID, outTmp, errTmp, validationResult, computingSystem, csStatus, metaData
jobDefinition field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, INT(16) NULL, VARCHAR(255), INT(16) NULL, ENUM('Defined'; 'Submitted'; 'Validated'; 'Failed'; 'Undecided'; 'Aborted') NULL, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(2048), VARCHAR(255), VARCHAR(255), VARCHAR(255), DATETIME, DATETIME, VARCHAR(255) NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL, VARCHAR(255) NULL, VARCHAR(255) NULL, TEXT NULL

transformation field names = identifier, name, version, runtimeEnvironmentName, arguments, outputFiles, script, comment, created, lastModified, inputFiles
transformation field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), VARCHAR(255), TEXT, DATETIME, DATETIME, VARCHAR(255)

runtimeEnvironment field names = identifier, name, computingSystem, certificate, url, initLines, depends, created, lastModified
runtimeEnvironment field types = INT(16) DEFAULT NULL AUTO_INCREMENT PRIMARY KEY, VARCHAR(255), VARCHAR(255), TEXT, VARCHAR(255), TEXT, TEXT, DATETIME, DATETIME

# This is to have an explicit file catalog

file field names = lfname, pfname, filetype, md5sum, fsize, lastmodified, archival, dsname, guid
pfns field = pfname

t_lfn field names = lfname, guid
t_lfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL

t_pfn field names = pfname, guid, filetype
t_pfn field types = VARCHAR(250) PRIMARY KEY, VARCHAR(40) DEFAULT NULL, VARCHAR(250) DEFAULT NULL

t_meta field names = guid, md5sum, fsize, lastmodified, archival, dsname, sync
t_meta field types = VARCHAR(40) PRIMARY KEY, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, BLOB DEFAULT NULL, VARCHAR(250), VARCHAR(40)

#--------------------------------------------------------------------
[ATLAS]
#
# Central ATLAS dataset and file catalog.
# The dataset catalog is the DQ2 server at CERN.
# The file catalog is a combination of the DQ2 server
# at CERN and the LFC and MySQL catalogs registered as
# sites in the TiersOfAtlas file at CERN. Writing file
# information is done by writing to the MySQL database
# registered as alias for the home LFC server.
# Authentication and authorization on this MySQL database
# is done via the grid certificate. The MySQL database is
# synchronized with the LFC database by a cron job running
# on the MySQL server.
#
class = gridpilot.dbplugins.atlas.ATLASDatabase

parameters = 

description = Central ATLAS dataset/file catalog
enabled = no

# Whether or not to cache search results locally
cache search results = yes

# The "home" LFC or MySQL server (its alias from TOA) and, for an LFC server,
# its optional MySQL URL alias; if the "home" server is an LFC server and
# no MySQL alias is given, there is only read access.
# If a MySQL alias is given, write access is obtained by not specifying any
# user:password in the URL. In this case the grid certificate is used for authentication.
#home site = FZKDISK mysql://dq2user:dqpwd@grid00.unige.ch:3306/localreplicas
home site = FZKDISK mysql://grid00.unige.ch:3306/localreplicas
# If the home site is an LFC site, the path under which new datasets should be
# registered should be specified. Notice that it will be prepended with /grid/atlas.
user path = /users/MyName
# When a file has several physical replicas, try these locations first.
preferred sites = NDGFT1DISK CSCS FZKDISK LYONDISK CERNCAF CERNPROD
# Sites we know are not responding.
ignored sites = NIKHEF PICDISK RALDISK TRIUMFDISK
# File catalog timeout in miliseconds
file catalog timeout = 10000

default dataset fields = dsn incomplete complete vuid
dataset identifier = vuid
dataset name = dsn
# Notice: because of DQ2 limitations: if you want to be able to download/replicate files
#         this list MUST be the full list: dsn lfn pfns guid
default file fields = dsn lfn catalogs pfns guid bytes checksum
file identifier = guid
file name = lfn
file dataset reference = dsn dsn
pfns field = pfns
bytes field = bytes
checksum field = checksum
# The file used to resolve site names to file catalog servers.
tiers of atlas = http://atlas.web.cern.ch/Atlas/GROUPS/DATABASE/project/ddm/releases/TiersOfATLASCache.py
#tiers of atlas = file:~/GridPilot/TiersOfATLASCache.txt
# If forceDelete is set to true, files will be attempted deleted on
# all physical locations and on the home catalog server MySQL alias
# and the home server will be de-registered in DQ, even if other
# catalog sites are registered in DQ than the home catalog or if there
# is no home catalog set.
force file deletion = no

## DQ2 parameters
DQ2 server = atlddmcat.cern.ch
DQ2 port = 80
DQ2 secure port = 443
DQ2 path = /dq2

#####################################################################
[Computing systems]
#
# Backends for executing computing jobs.
#

systems = FORK SSH SSH_POOL EC2 GPSS NG GLite

# Maximum number of simultaneous submission threads.
maximum simultaneous submissions = 3
# Delay between starting submissions (in milliseconds).
time between submissions = 10 # in ms
# Whether to start submissions in the order they appear in the table
# or in random order.
randomized submission = no
# Delay between starting validation (in milliseconds)
delay before validation = 3000
# Maximum number of simultaneous validation threads.
maximum simultaneous validating = 3
#--------------------------------------------------------------------
[FORK]
#
# Run jobs on the local computer.
#

## Sub-section for GridPilot
enabled = yes
host = localhost
user =
password =
class = gridpilot.csplugins.fork.ForkComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking)
max jobs by update = 10

## Sub-section for this plug-in
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fall-back shell command for getting remote input files.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Directory with runtime setup scripts.
runtime directory = ~/GridPilot/runtimeEnvironments
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DBs to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Local

# Certificate used by submitters to encrypt their
# proxy certificate+key to the GridPilot picking up the job.
# Only needed to pick up jobs.
# NOTICE: the certificate should NOT include a key!
public certificate = ~/.globus/usercert.pem
# Remote DB to announce runtime environments, upload certificate and
# from where to pick up jobs.  Only needed to pick up jobs.
remote pull database =
# Max number of running jobs pulled from remote database.
# Only used when picking up jobs.
max pulled running jobs = 2

#--------------------------------------------------------------------
[GPSS]
#
# Submit jobs to a GridPilot database somewhere,
# where they will be picked up by other GridPilots.
#

## Sub-Section for GridPilot
enabled = no
class = gridpilot.csplugins.gpss.GPSSComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 40

## Sub-Section for this plug-in
# Where to write the jobDefinitions.
remote database = My_DB_Remote
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Remote
# Where to upload RTE tarballs (to make them available to pull clients), job scripts
# and local input files and where to have pull providers write stdout and stderr.
# Make sure this directory exists!
remote directory = gsiftp://grid01.unige.ch/grid/users/fjob/gpss/
# Whether or not for a given job to copy the RTE tarballs to the remote directory
# (and change the runtimeEnvironment record correspondingly)
rte proxy = true
# Space separated list of certificate subjects or URLs (http, ftp, gsiftp, srm, file)
# of files containing certificate subjects, e.g. 'http://www.chipp.ch/vo/chipp.txt'.
# Each list entry must be enclosed in single-quotes.
# The owner of a certificates with one these subjects will be allowed
# to run jobs, if the CA that issued the certificate is on the list of ca certificates.
# If empty, any provider will be allowed to run my jobs.
allowed subjects = 
# Number of seconds since last update by provider before a job is considered lost
provider update timeout = 300

#--------------------------------------------------------------------
[SSH]
#
# Run jobs on remote computer.
#

## Sub-section for GridPilot
enabled = no
# Notice: if you use lxplus.cern.ch you disconnect, reconnect and find you jobs again,
# since you will most likely be on a different machine.
host = lxplus203.cern.ch
user = 
password =
class = gridpilot.csplugins.fork.ForkComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1

## Sub-Section for this plug-in
# Optionally login with a private key. This will be tried only once, then
# normal login with password will be tried
SSH key file = 
SSH key passphrase = 
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Directory with runtime setup scripts.
runtime directory = ~/GridPilot/runtimeEnvironments
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Remote

#--------------------------------------------------------------------
[SSH_Pool]
#
# Run jobs on a pool of remote computers.
#

## Sub-section for GridPilot
enabled = no
# The main host, where runtime environments, etc. are present.
host = localhost
user = 
password =
class = gridpilot.csplugins.fork.ForkPoolComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1

## Sub-Section for this plug-in
# Optionally login with a private key. This will be tried only once, then
# normal login with password will be tried
SSH key file = 
SSH key passphrase = 
# Additional hosts for running jobs
hosts = lxplus218.cern.ch lxplus219.cern.ch
# List of user names (one per host). If empty prompting will be done.
users = 
# List of passwords (one per host). If empty prompting will be done.
passwords =
# Maximum number of running jobs (one number for each host). If empty, 1's will be assumed.
max running jobs =
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Directory with runtime setup scripts.
runtime directory = ~/GridPilot/runtimeEnvironments
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Remote

#--------------------------------------------------------------------
[EC2]
#
# Run jobs on a pool of virtual machines in the Amazon Elastic
# Compute Cloud.
#

## Sub-section for GridPilot
enabled = no
# host should just be localhost. It is used for manipulating local job
# information, etc.
host = localhost
# user should be left blank - we're on localhost
user = 
# password should be left blank - we're on localhost
password =
class = gridpilot.csplugins.ec2.EC2ComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1

## Sub-Section for this plug-in
# The access key id found on your AWS account web page
aws access key id =
# The secret access key found on your AWS account web page
aws secret access key =
# The subnet from which ssh access to the AWS virtual machines will
# be granted. If left empty, 0.0.0.0/0 is used, i.e. access is granted from anywhere
ssh access subnet = 0.0.0.0/0
# Maximum number of virtual machines to be run
maximum machines = 2
# The AWS Machine Image (AMI) to use
ami id = ami-20b65349 # ec2-public-images/fedora-core4-base.manifest.xml available amazon
# Number of jobs per machine.
jobs per machine = 1
# Directory to 'cd' to before running job scripts.
working directory = ~/GridPilot/jobs
# Fallback command for remote input files or output destination
# (castor://, ftp://, gsiftp://, ...), e.g. ngcp gsiftp://server/dir/file.root file.root.
# Optionally a runtime environment providing this command should be specified.
# Notice: to have file copied from the local machine to the ssh host, specify
# input files as file://. If they are given as /dir/file or C:\dir\file, they
# are assumed to be on the server already.
remote copy command =
# This RTE script will be sourced regardless of whether it is specified
# in the job record or not. Typically used to provide the 'remote copy command'.
required runtime environment =
# Directory with runtime setup scripts.
runtime directory = ~/GridPilot/runtimeEnvironments
# Where to create a sample test transformation. If not defined, none is created.
transformation directory = ~/GridPilot/transformations
# In which DB to write the runtime environment records. If not specified,
# runtime environment records must be defined by hand.
runtime databases = My_DB_Remote

#--------------------------------------------------------------------
[NG]
#
# Submit jobs to NorduGrid.
#

## Sub-section for GridPilot
enabled = no
class = gridpilot.csplugins.ng.NGComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1

## Sub-section for this plug-in
# This is where scripts and stdout/stderr is kept on the *local* disk.
# Use / instead of \.
working directory = ~/GridPilot/jobs
# Shell to write in header of job scripts.
shell = /bin/sh
# Where to write the runtime records.
runtime databases = My_DB_Local My_DB_Remote
# Whether or not to use the information system.
# If set to no, functionality is somewhat limited, but
# it should be possible to submit jobs to a specific cluster.
use information system = yes
# The GIIS information servers to query for clusters.
GIISes = ldap://index1.nordugrid.org:2135/Mds-Vo-name=nordugrid,o=grid # ldap://odin.switch.ch:2135/Mds-Vo-name=Switzerland,o=grid
# Explicitly chosen clusters. If this is set, GIIS servers will not be queried.
clusters = grid02.unige.ch # grid00.unige.ch lheppc50.unibe.ch lheppc10.unibe.ch nordugrid-lcg.projects.cscs.ch hypatia.uio.no hagrid.it.uu.se benedict.grid.aau.dk gateway01.dcsc.ku.dk interop.dcgc.dk morpheus.dcgc.dk # grid01.unige.ch nordugrid.unibe.ch
# Required CPU time in minutes.
cpu time = 180
# Maximum number of retrying failed jobs.
max rerun = 1

#--------------------------------------------------------------------
[GLite]
#
# Submit jobs to gLite.
#

## Sub-section for GridPilot
enabled = no
class = gridpilot.csplugins.glite.GLiteComputingSystem
# Maximum number of jobs whose status is checked by each thread
# (see maximum simultaneous checking).
max jobs by update = 1

## Sub-section for this plug-in
# This is where scripts and stdout/stderr is kept on the *local* disk.
# Use / instead of \.
working directory = ~/GridPilot/jobs
# Shell to write in header of job scripts.
shell = /bin/sh
# The virtual organization under which jobs will run
# (the VirtualOrganisation tag in the JDL).
virtual organization = atlas
# Required CPU time in minutes.
cpu time = 180
# Maximum number of retrying failed jobs.
max rerun = 1
# The service endpoint.
wmproxy url = https://rb.grid.upjs.sk:7443/glite_wms_wmproxy_server # https://rb103.cern.ch:7443/glite_wms_wmproxy_server
# The BDII host.
bdii host = rb.grid.upjs.sk # lcg-bdii.cern.ch
# Clusters to scan for RTEs. Leaving this empty will cause all clusters
# to be scanned - this may take a long time.
runtime clusters = ce.grid.upjs.sk # ce01-lcg.projects.cscs.ch g03n02.pdc.kth.se
# Space separated list of virtual organizations for which to setup RTEs.
runtime vos = ATLAS CMS
# RTE tags to be used to label replacement rules. Can be any strings.
runtime tags = atlas_production
# For each of above tags. Mapping of tag to actual setup scripts to be sourced.
# The first string must be a regular expression, the following strings must resolve to
# the script paths.
atlas_production = VO-atlas-production-(.*) \$VO_ATLAS_SW_DIR/software/$1/setup.sh \$SITEROOT/AtlasOffline/$1/AtlasOfflineRunTime/cmt/setup.sh
# Where to write the runtime records.
runtime databases = My_DB_Local

# This entry must be here for PreferencesPanel to work.
[end]
